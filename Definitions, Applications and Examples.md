| **Sr. No.** | **Topic**               | **What it is? (wrt Data Science and ML)**                                                                                                      | **What it is used for?**                                                                                                            | **How it is used?**                                                                                                                                        | **Real-Life Examples**                                                                                                      |
|-------------|--------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|
| 1           | **Limit**                  | A fundamental concept in calculus that describes the value a function approaches as the input approaches a specific point.                     | Used to define derivatives and continuity in machine learning models, especially for gradient computation and convergence analysis.  | Applied in understanding the behavior of cost functions near minima or in analyzing the stability of optimization algorithms.                              | Convergence of gradient descent in ML, or evaluating numerical stability in simulations.                                   |
| 2           | **Calculus**              | A branch of mathematics that studies continuous change, focusing on limits, derivatives, integrals, and infinite series.                      | Used for optimization, cost function analysis, and gradient-based learning algorithms like gradient descent.                        | Applied in machine learning for optimization, where gradients are computed to minimize loss functions (e.g., neural networks, linear regression).              | Optimizing machine learning models, solving differential equations in physics-based simulations.                           |
| 3           | **Multivariate Calculus** | An extension of calculus to functions of multiple variables, focusing on partial derivatives, gradients, and optimization in higher dimensions. | Used for optimizing multivariable functions in machine learning, such as loss functions in multi-dimensional feature spaces.       | Applied in gradient-based optimization algorithms, like gradient descent in multivariable settings, used for training complex models like neural networks.      | Training neural networks with multiple features, multivariable regression models.                                          |
| 4           | **Functions**             | A relationship between input and output where each input is related to exactly one output. In ML, they represent models or transformations.   | Used to represent machine learning models, data transformations, or mappings between features and target variables.                | Applied in defining machine learning models like linear regression `y = f(x)` or mapping features to predictions in supervised learning.               | Predictive models in ML (e.g., predicting house prices based on features like area, location).                            |
| 5           | **Gradients**             | The vector of partial derivatives of a function with respect to its variables, indicating the direction of the steepest ascent.             | Used in optimization to find the direction in which a function increases or decreases most rapidly.                                | Applied in gradient descent algorithms to minimize a cost function by moving in the opposite direction of the gradient.                                      | Training deep learning models by adjusting weights based on gradients to minimize loss.                                    |
| 6           | **Derivatives**           | The rate of change of a function with respect to one of its variables, describing how a function changes as its input changes.             | Used to compute gradients, optimize functions, and understand the sensitivity of models to changes in input.                       | Applied in backpropagation for neural networks, or in gradient-based optimization algorithms to find optimal model parameters.                                | Finding the slope of cost functions in regression or calculating the rate of change in physical systems (e.g., velocity). |
| 7           | **Sum Rule**               | A rule in differentiation stating that the derivative of a sum of functions is the sum of their derivatives.                                   | Used to simplify gradient computations for complex models, such as those involving sums of multiple loss terms or feature effects.  | Applied in calculating gradients of models with additive components, like ensemble methods or composite loss functions.                                     | Training ensemble models like gradient-boosted trees or calculating the gradient of a combined loss in multi-task learning. |
| 8           | **Product Rule**           | A rule for finding the derivative of the product of two functions, stating `d/dx [f(x) * g(x)] = f'(x) * g(x) + f(x) * g'(x)`.                                             | Used to compute gradients of cost functions involving multiplicative terms, such as interaction effects or compound expressions.    | Applied in backpropagation where functions involve products of parameters, such as weights and activations in neural networks.                              | Calculating gradients in neural networks with multiplicative relationships between layers.                                 |
| 9           | **Power Rule**             | A differentiation rule stating that the derivative of `x^n = n * x^(n-1)`.                                                      | Used for gradient computations in polynomial loss functions or any power-related model equations.                                   | Applied in calculating gradients for regularization terms, such as `L_2` -norm penalties in regression or optimization.                                   | Training models with polynomial regression or applying regularization in ML.                                               |
| 10           | **Chain Rule**             | A rule for finding the derivative of a composite function, stating `d/dx [f(g(x))] = f'(g(x)) * g'(x)`.                               | Used in backpropagation to compute gradients of nested functions in deep learning.                                                  | Applied in training deep neural networks, where gradients flow through layers of composite functions.                                                     | Backpropagation in deep learning or optimizing nested functions in reinforcement learning.                                 |
| 11          | **Quotient Rule** | A rule for differentiating a quotient of two functions, given by `d/dx [f(x) / g(x)] = (f'(x) * g(x) - f(x) * g'(x)) / (g(x))^2`.                     | Used in optimization and gradient calculations when dealing with division or ratios in functions, such as likelihoods or probabilities. | Applied in backpropagation and optimization tasks where the cost or objective function involves ratios or divisions.                                        | Computing gradients in logistic regression, or in probabilistic models with likelihood functions involving divisions.      |
| 12          | **Total Derivative**       | The derivative of a function considering all paths of dependency, accounting for both direct and indirect influences of variables.              | Used to analyze systems where variables are interdependent, such as multivariate optimization or complex ML models.                 | Applied in analyzing the sensitivity of outputs to changes in all related inputs, including secondary relationships.                                       | Optimizing interdependent parameters in ML or understanding sensitivity in economic models.                               |
| 13          | **Partial Differentiation** | A method of differentiation used for functions of multiple variables, focusing on the rate of change of a function with respect to one variable, while keeping others constant. | Used in optimization and gradient-based algorithms where a function depends on multiple variables (e.g., loss functions).          | Applied in computing gradients for optimization algorithms like gradient descent when there are multiple variables involved in the cost function.            | Training machine learning models where the cost function depends on multiple parameters, such as in linear regression or neural networks. |
| 14          | **Jacobian**              | A matrix of all first-order partial derivatives of a vector-valued function. It describes the local linear approximation of a function.          | Used in optimization, backpropagation, and understanding how small changes in input affect the output of a multi-dimensional function. | Applied in training deep learning models, optimization of multi-variable functions, or understanding how each feature contributes to the model's output.      | Backpropagation in neural networks, optimizing multi-variable functions in reinforcement learning.                        |
| 15           | **Hessians**              | A square matrix of second-order partial derivatives of a function, which helps determine the curvature or concavity of the function.              | Used in optimization algorithms, especially in Newtonâ€™s method, to understand the curvature of a function and guide optimization.    | Applied in second-order optimization methods like Newtonâ€™s method to update model parameters more efficiently by considering the curvature of the cost function. | Training deep learning models with second-order optimization methods, analyzing cost functions for concavity or convexity. |
| 16           | **Multivariate Chain Rule** | An extension of the chain rule for functions of multiple variables, where dependencies between variables are considered.                        | Used in gradient-based optimization in high-dimensional spaces, like those encountered in deep learning.                            | Applied in computing gradients for multi-variable models by accounting for all partial derivatives and their interactions.                                   | Training neural networks with multiple features or optimizing multi-variable cost functions in ML.                        |
| 17 | **Artificial Neural Network (ANN)**         | A computational model inspired by biological neural networks, consisting of interconnected layers of neurons. | Used for tasks like classification, regression, and pattern recognition by mimicking the way humans learn.                   | Configured with input, hidden, and output layers, and trained using algorithms like backpropagation.                      | Image recognition, NLP tasks, and recommendation systems.                                              |
| 18 | **Activity in Neural Network (a)**              | The output or activation value of a neuron after applying the activation function to the weighted input sum.   | Represents the processed signal passed to the next layer or used as the network output.                                     | Computed as `a = activation(w * x + b)` where `w` is the weight, `x` is the input, and `b` is the bias.                   | Detecting features in convolutional layers during image classification.                                |
| 19 | **Weight in Neural Network (W)**                | Parameters that define the importance of input connections between neurons.                                   | Used to learn patterns and relationships in the data by adjusting during training to minimize error.                        | Weights are updated iteratively using optimization algorithms like gradient descent.                                     | Capturing complex patterns for tasks like language translation or speech recognition.                   |
| 20 | **Activation Threshold**                    | The minimum value required for a neuron to activate or pass information forward in a neural network.           | Controls whether a neuron contributes to the output, enabling selective activation and filtering of irrelevant inputs.        | Often determined by adding a bias term to the weighted sum of inputs: `z = w * x + b`.                                       | Used in binary classification (e.g., sigmoid outputs above 0.5 indicate a positive class).                                   |
| 21 | **Bias in Neural Network (b)**                  | An additional parameter added to the input of a neuron to allow flexibility in the activation threshold.       | Helps shift the activation function, improving model accuracy and ensuring better representation of data.                    | Bias is added as `z = w * x + b`, allowing the model to fit more complex data better.                                     | Ensuring models like logistic regression or neural networks learn non-zero thresholds.                  |
| 22 | **ReLU (Rectified Linear Unit)**             | A widely used activation function in neural networks that outputs the input if positive, otherwise zero.       | Introduces non-linearity and helps the network learn complex patterns while keeping computations efficient.                  | Computed as `ReLU(x) = max(0, x)`. Commonly used in convolutional and feedforward layers.                                     | Applied in tasks like image classification, object detection, and natural language processing.                                |
| 23 | **Activation Function (Sigma) in Neural Network**   | A non-linear function applied to neuron outputs to introduce complexity and allow the network to learn patterns. | Helps model non-linear relationships and allows stacking of layers in deep networks.                                       | Examples include `ReLU(x) = max(0, x)`, `Sigmoid(x) = 1 / (1 + e^(-x))`, or `Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))`.   | ReLU in CNNs for object detection or Sigmoid in logistic regression tasks.                           |
| 24 | **Cost Function in Neural Network (C)**         | A function that evaluates the error between predicted and actual outputs.                                     | Guides the optimization process by quantifying the performance of the model.                                                | Examples: Mean Squared Error (MSE) = `1/n * sum((y_pred - y_actual)^2)` or Cross-Entropy Loss = `-sum(y_actual * log(y_pred))`. | Loss functions in tasks like predicting house prices or classifying images.                             |
| 25 | **Stochastic Gradient Descent (SGD)**       | An optimization algorithm that updates weights iteratively for a small random batch of training samples.       | Used for efficient training of large datasets in neural networks.                                                           | Updates weights as `w_new = w_old - learning_rate * gradient` for each mini-batch of data.                               | Training models like ResNet for image recognition or BERT for NLP tasks.                                |
| 26 | **Backpropagation in Neural Network**       | An algorithm for training neural networks by calculating gradients of the cost function and updating weights.  | Allows efficient computation of weight updates by propagating error backwards through the network.                           | Uses the chain rule to compute gradients: `âˆ‚C/âˆ‚w = âˆ‚C/âˆ‚a * âˆ‚a/âˆ‚z * âˆ‚z/âˆ‚w`, where `C` is cost, `a` is activation, and `z` is input. | Training deep networks for tasks like object detection, recommendation systems, and language modeling.   |
| 27 | **Approximation / Approximate Complex Functions** | The process of estimating complex functions using simpler representations like polynomials or other models.               | Used to simplify calculations and provide solutions where exact computation is infeasible.                                    | Techniques include Taylor series, Fourier series, and regression models.                                                     | Applications in numerical analysis, machine learning, and scientific simulations.                                             |
| 28 | **Taylor Series**                        | A mathematical representation of a function as an infinite sum of terms derived from its derivatives at a single point.  | Used for approximating complex functions with polynomials for easier computation.                                             | The function is expressed as `f(x) â‰ˆ f(a) + f'(a)(x-a) + f''(a)(x-a)^2/2! + ...` centered around point `a`.                  | Applications in physics simulations, computer graphics, and solving differential equations.                                   |
| 29 | **Power Series**                         | An infinite series in the form `Î£ c_n * (x-a)^n` that represents a function within its radius of convergence.             | Used to approximate functions, solve differential equations, and model complex systems.                                       | Expressed as `f(x) = Î£ c_n * (x-a)^n` where `c_n` are coefficients and `a` is the center of the series.                     | Common in numerical analysis, signal processing, and modeling in physics and engineering.                                    |
| 30 | **Maclaurin Series**                     | A special case of the Taylor Series where the series is expanded around `a = 0`.                                          | Used for function approximations and simplifying complex expressions.                                                        | Expressed as `f(x) â‰ˆ f(0) + f'(0)x + f''(0)x^2/2! + ...`.                                                                    | Calculating approximations like `e^x`, `sin(x)`, and `cos(x)` in computational tasks.                                        |
| 31 | **Linearisation**                        | The process of approximating a non-linear function with a linear function near a given point.                             | Simplifies complex problems by providing a local linear approximation.                                                       | Linearised as `f(x) â‰ˆ f(a) + f'(a)(x-a)` for small deviations around `a`.                                                   | Used in control systems, optimization, and solving non-linear differential equations.                                        |
| 32 | **Multivariate Power Series**            | A generalization of the power series for functions with multiple variables.                                               | Used to approximate multivariable functions and analyze their behavior near a given point.                                    | Expressed as `f(x, y) = Î£ c_{ij} * (x-a)^i * (y-b)^j` for variables `x` and `y` around `(a, b)`.                             | Modeling in multivariate calculus, solving partial differential equations, and fitting surfaces in data analysis.            |
| 33 | **2D Taylor Series**                     | A mathematical method for approximating a multivariable function using polynomials based on its partial derivatives.     | Used to approximate functions of two variables locally for analysis, simulation, and optimization.                            | Expressed as `f(x, y) â‰ˆ f(a, b) + f_x(a, b)(x-a) + f_y(a, b)(y-b) + (1/2!)[f_xx(a, b)(x-a)^2 + 2f_xy(a, b)(x-a)(y-b) + f_yy(a, b)(y-b)^2] + ...`. | Applications in numerical analysis, engineering simulations, and multivariable optimization problems.                        |
| 34 | **Optimisation using Multivariate Calculus** | The process of finding the minima or maxima of a function using derivatives and gradients.                                  | Used to solve problems in machine learning, economics, and engineering where optimal solutions are required.                  | Involves setting the gradient to zero and solving for critical points: `âˆ‡f(x) = 0`.                                          | Applications in training machine learning models, resource allocation, and physics simulations.                              |
| 35 | **Minimum and Maximum of a Function**       | Points where a function reaches its smallest or largest value locally or globally.                                         | Helps in identifying optimal values in various fields like machine learning and optimization problems.                        | Critical points are found by solving `âˆ‡f(x) = 0`, and second derivatives are used to classify them.                         | Used in economics for cost optimization or profit maximization, and physics for equilibrium analysis.                        |
| 36 | **Newton-Raphson Method**                   | An iterative numerical method for finding roots of a real-valued function.                                                 | Used to solve equations, optimize functions, and refine solutions in computational tasks.                                     | Iteratively updated as `x_{n+1} = x_n - f(x_n)/f'(x_n)`.                                                                    | Applied in training machine learning models and solving non-linear equations in simulations.                                 |
| 37 | **Grad (Vector of Jacobian)**               | The gradient vector, containing the first-order partial derivatives of a multivariable function.                           | Represents the direction and rate of steepest ascent of the function.                                                        | `âˆ‡f(x, y, z) = [âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y, âˆ‚f/âˆ‚z]`.                                                                                     | Used in gradient-based optimization methods, such as gradient descent, in machine learning.                                  |
| 38 | **Minima and Maxima**                       | Points in a function where it attains the smallest or largest value in a local or global sense.                            | Important in optimization tasks to find the best or worst possible outcome of a process.                                      | Found by analyzing the gradient (`âˆ‡f(x) = 0`) and Hessian matrix for second-order behavior.                                 | Applications in machine learning, economics, and physics for optimization and decision-making.                              |
| 39 | **Gradient Descent**                        | An optimization algorithm that minimizes a function by moving in the negative gradient direction.                          | Used in training machine learning models and minimizing cost functions.                                                       | Updates parameters as `Î¸_new = Î¸_old - Î± * âˆ‡f(Î¸)` where `Î±` is the learning rate.                                           | Common in training neural networks and regression models.                                                                    |
| 40 | **Lagrange Multipliers Method**             | A technique for finding local minima and maxima of a function subject to equality constraints.                             | Used to optimize constrained problems in engineering, physics, and economics.                                                | Solves `âˆ‡f(x, y, ...) + Î»âˆ‡g(x, y, ...) = 0`, where `g(x, y, ...)` is the constraint.                                         | Applied in resource allocation, structural design, and optimization under constraints.                                       |
| 41 | **Linear Function**                        | A function whose graph is a straight line and can be expressed in the form `f(x) = mx + b`.                               | Used to model relationships with constant rates of change and in basic regression tasks.                                     | Represents a proportional relationship, where `m` is the slope and `b` is the y-intercept.                                   | Common in linear regression for predicting trends or financial modeling.                                                     |
| 42 | **Non-Linear Function**                    | A function whose graph is not a straight line and has varying rates of change.                                            | Used to model more complex relationships that cannot be represented by a straight line.                                       | Includes functions like polynomials, exponentials, and trigonometric equations: `f(x) = x^2 + e^x`.                          | Used in neural networks, population modeling, and complex systems analysis.                                                  |
| 43 | **Simple Linear Regression**               | A statistical method for modeling the relationship between a dependent variable and a single independent variable.        | Used for predicting outcomes and finding trends in data.                                                                     | Expressed as `y = mx + b`, where `m` is the slope and `b` is the intercept, fitted using the least squares method.           | Predicting sales, analyzing trends in economics, or estimating home prices.                                                  |
| 44 | **Non-Linear Regression**                  | A regression technique for modeling relationships between variables when the data does not follow a straight line.         | Used to capture complex relationships and patterns in data.                                                                  | Expressed with non-linear models like `y = a * e^(bx)` or `y = a * x^b`, solved using optimization methods.                  | Applications in growth modeling, pharmacokinetics, and machine learning.                                                     |
| 45 | **Least Squares**                          | A method for minimizing the sum of the squared differences between observed and predicted values.                         | Used to find the best-fitting line or curve for a given set of data.                                                         | Solves `min Î£(y_actual - y_predicted)^2` by adjusting parameters to minimize error.                                          | Widely used in linear regression, trend analysis, and calibration of scientific instruments.                                  |
| 46 | **Non-Linear Least Squares**               | A variant of least squares for fitting non-linear models to data by minimizing the sum of squared residuals.              | Used to fit non-linear functions to observed data in various scientific and engineering applications.                        | Solved iteratively using optimization algorithms like Gauss-Newton or Levenberg-Marquardt methods.                          | Used in fitting exponential growth curves, modeling enzyme kinetics, and signal processing.                                  |
| 47 | **Fitting a Non-Linear Function**          | The process of determining the parameters of a non-linear model that best fits a given set of data.                       | Used in data analysis and scientific research to model complex systems.                                                      | Achieved by minimizing the residuals between observed and predicted values, often using non-linear least squares methods.    | Applied in curve fitting, machine learning, and modeling natural phenomena like weather or biological systems.               |
